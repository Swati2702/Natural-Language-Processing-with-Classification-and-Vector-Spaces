# Natural-Language-Processing-with-Classification-and-Vector-Spaces
Course 1 of Coursera Natural Language Processing Specialization


<h2> Assignment 1: Logistic Regression </h2>

Welcome to week one of this specialization. You will learn about logistic regression. <br>
Concretely, you will be implementing logistic regression for sentiment analysis on tweets. 

Given a tweet, you will decide if it has a positive sentiment or a negative one. Specifically you will:

Learn how to extract features for logistic regression given some text
Implement logistic regression from scratch
Apply logistic regression on a natural language processing task
Test using your logistic regression
Perform error analysis


<h2> Assignment 2: Naive Bayes </h2>
Welcome to week two of this specialization. You will learn about Naive Bayes. Concretely, you will be using Naive Bayes for sentiment analysis on tweets. Given a tweet, you will decide if it has a positive sentiment or a negative one. Specifically you will:

Train a naive bayes model on a sentiment analysis task
Test using your model
Compute ratios of positive words to negative words
Do some error analysis
Predict on your own tweet
You may already be familiar with Naive Bayes and its justification in terms of conditional probabilities and independence.

In this week's lectures and assignments we used the ratio of probabilities between positive and negative sentiments.
This approach gives us simpler formulas for these 2-way classification tasks.


<h2> Assignment 3: Hello Vectors </h2>
Welcome to this week's programming assignment on exploring word vectors. In natural language processing, we represent each word as a vector consisting of numbers. The vector encodes the meaning of the word. These numbers (or weights) for each word are learned using various machine learning models, which we will explore in more detail later in this specialization. Rather than make you code the machine learning models from scratch, we will show you how to use them. In the real world, you can always load the trained word vectors, and you will almost never have to train them from scratch. In this assignment, you will:

Predict analogies between words.
Use PCA to reduce the dimensionality of the word embeddings and plot them in two dimensions.
Compare word embeddings by using a similarity measure (the cosine similarity).
Understand how these vector space models work.


<h2> Assignment 4 - Naive Machine Translation and LSH </h2>
You will now implement your first machine translation system and then you will see how locality sensitive hashing works. Let's get started by importing the required functions!
